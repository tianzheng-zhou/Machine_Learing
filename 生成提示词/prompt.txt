 你是一个专业程序员，请你阅读下面的python文件，给我讲解这些文件分别是用来做什么的

我已经将这些文件的文件名标注在内容的前面。


## File: args.py
```
import numpy as np
from typing import Callable
import random
import multiprocessing
import torch
episode_length=50   #the length of single episode
gamma=0.95            #loss rate
batch_length=100
update_round=5
AN_cache_min_length=200 #the minimum length of cache from sample processing to active network

sample_threading_num=3

run_round=300
convergence_critirion=0.01
reward_common_road=0
reward_forbidden_area=-1
reward_target=10

lr_rate=0.02

max_row=1
max_column=1
stop_flag_AN=multiprocessing.Value('i',0)
stop_flag_SP=multiprocessing.Value('i',0)
stop_flag_TN=multiprocessing.Value('i',0)
def random_choice(length):
    def the_policy(action_values):
        return np.array([1/length]*length)
    return the_policy
def epsilon_greedy(epsilon:float)->Callable[[np.ndarray,],np.ndarray]: #give a function giving the probability choosing actions under epsilon-greedy policy.Used in <class policy>.__init__
    def the_policy(action_values:np.ndarray):
        cnt=len(action_values)
        p=np.array([epsilon/cnt]*cnt)
        p[np.argmax(action_values)]+=1-epsilon
        return p
    return the_policy
device = torch.device(
    "cuda" if torch.cuda.is_available() else
    "mps" if torch.backends.mps.is_available() else
    "cpu"
)
```

## File: classes.py
```
import numpy as np
import random
from numba import jit
import torch
import torch.nn as nn
import multiprocessing
from typing import Callable
import args

global space
space = []


class block:
    def __init__(self, actions: list):
        self.position = 0
        self.state_value = 0
        self.state_reward = 0
        self.actions = actions
        self.action_values = np.array([0] * len(actions))


class set():
    def __init__(self, blocks: list, *maxlen):
        self.blocks = list(blocks)
        self.maxlen = list(maxlen)
        prod = 1
        for i in maxlen:
            prod *= i
        self.length = prod
        cnt = 0
        for i in self.blocks:
            i.postion = cnt
            cnt += 1

    def __len__(self):
        return self.length

    def __iter__(self):
        return self.blocks.__iter__()

    def __getitem__(self, index):
        return self.blocks[index]

    def getitem(self, *keys):
        position = 0
        base = 1
        for i in range(len(self.maxlen) - 1, -1, -1):
            position += keys[i] * base
            base *= self.maxlen[i]
        return self.blocks[position]

    def get_index(self, *keys):
        position = 0
        base = 1
        for i in range(len(self.maxlen) - 1, -1, -1):
            position += keys[i] * base
            base *= self.maxlen[i]
        return position


class action:
    def __init__(self, next_state: list, next_state_p: np.ndarray):
        self.action_value = 0
        self.action_reward = 0
        self.visit_cnt = 0  #times that be visited
        self.next_state = next_state
        self.next_state_p = next_state_p
        self.direction = []

    #self.next_state_p_expected=np.array([1/len(next_state)]*len(next_state)) #the expected position after some action used for questions that the next state is not settled after a certain action
    def __iter__(self):
        return self

    def __next__(self) -> int:  #giving next state by taking this action
        total = np.sum(self.next_state_p)
        x = random.random() * total
        for i in range(len(self.next_state_p)):
            x -= self.next_state_p[i]
            if x <= 0:
                return self.next_state[i]
        return self.next_state[0]

    def update(self, ret, a: Callable[[int, ], float] = lambda x: 1 / x):  # 'a' is the conerge coiffiecient in RM
        self.visit_cnt += 1
        self.action_value = self.action_value - a(self.visit_cnt) * (self.action_value - ret)


class policy:
    def __init__(self, principle: Callable[[np.ndarray, ], np.ndarray]):
        self.principle = principle

    def __call__(self, state: block) -> np.ndarray:  #call the instance then get probability of chocing actions
        values = list(map(lambda x: x.action_value, state.actions))
        return self.principle(values)

    def choice(self, state: block) -> action:  #return the action the policy choose
        probability = self.__call__(state)
        total = probability.sum()
        x = total * random.random()
        for i in range(len(probability)):
            x -= probability[i]
            if x <= 0:
                return i
        return 0


class episode:
    def __init__(self, state: block, policy: policy,
                 step: int):  # use a initial state to create an episode with lenth of 'step' by the policy
        self.track = []
        self.now_state = state
        self.policy = policy
        if step < 0:
            while space[self.now_state].state_reward <= 0:
                a = policy.choice(space[self.now_state])
                #a=self.act(policy)
                self.track.append((self.now_state, a))
                self.now_state = next(space[self.now_state].actions[a])
            return
        for i in range(step):
            a = policy.choice(space[self.now_state])
            #a=self.act(policy)
            self.track.append((self.now_state, a))
            if space[self.now_state].state_reward > 0:
                return
            self.now_state = next(space[self.now_state].actions[a])

    def __iter__(self) -> list:
        return self.track

    def act(self, policy: policy) -> action:  # return the action the policy choose  ,same as choice in class policy
        return policy.choice()


class QN(nn.Module):
    def __init__(self, state_dim, action_dim):
        super(QN, self).__init__()
        self.fc1 = nn.Linear(state_dim, 128)
        self.fc2 = nn.Linear(128, 128)
        self.fc3 = nn.Linear(128, 128)
        self.fc4 = nn.Linear(128, 128)
        self.fc5 = nn.Linear(128, 256)
        self.fc6 = nn.Linear(256, action_dim)
        torch.nn.init.uniform_(self.fc1.weight, -0.15, 0.15)
        torch.nn.init.uniform_(self.fc2.weight, -0.15, 0.15)
        torch.nn.init.uniform_(self.fc3.weight, -0.15, 0.15)
        torch.nn.init.uniform_(self.fc4.weight, -0.15, 0.15)
        torch.nn.init.uniform_(self.fc5.weight, -0.15, 0.15)
        torch.nn.init.uniform_(self.fc6.weight, -0.15, 0.15)
        self.function = nn.functional.leaky_relu
        #self.inp,self.out=multiprocessing.Pipe(True)
        #self.inp=inp_queue
        self.criterion = torch.nn.SmoothL1Loss()  #reduction='mean')
        self.optimizer = torch.optim.SGD(self.parameters(), args.lr_rate)

    def forward(self, x):
        x = self.function(self.fc1(x))
        x = self.function(self.fc2(x))
        x = torch.nn.functional.sigmoid(self.fc3(x))
        x = self.function(self.fc4(x))
        x = self.function(self.fc5(x))
        return self.fc6(x)

    def train(self, mini_batch):
        batch_length = len(mini_batch)
        sa, action_value = zip(*mini_batch)
        # next_states = np.array(next_states)
        pred = torch.zeros(batch_length, device=args.device)
        for epoch in range(batch_length):
            pred[epoch] = self.forward(torch.tensor(sa[epoch]).to(args.device))
            # print(f"TN sa:{sa[epoch]}")
            # print(f"TN pred:{pred}")
            # print(f"TN action_value{action_value[epoch]}")
            # print(f"TN av tensor{torch.tensor((action_value[epoch],)).to(args.device)}")
        loss = self.criterion(pred, torch.tensor((action_value)).to(args.device))
        self.optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_value_(self.parameters(), 100)
        self.optimizer.step()
        print(f"loss:{loss}")
        return 1

```

## File: Dqn.py
```
import multiprocessing.managers
import multiprocessing.queues
import queue
import threading
from concurrent.futures import ThreadPoolExecutor
import torch
import torch.nn as nn
import multiprocessing
import random
import args
import classes
import numpy as np
import copy
import time
import matplotlib.pyplot as plt

Net_copy = multiprocessing.Queue(5)


def train(self, mini_batch, losses):
    #criterion = torch.nn.MSELoss(reduction='mean')   
    criterion = torch.nn.SmoothL1Loss()
    optimizer = torch.optim.SGD(self.parameters(), lr=args.lr_rate)
    batch_length = len(mini_batch)
    sa, action_value = zip(*mini_batch)
    sa = list(sa)
    action_value = list(action_value)

    #next_states = np.array(next_states)
    pred = []
    #pred=torch.zeros(batch_length,device=args.device)

    action_value = torch.cat(action_value)
    #pred=self(torch.cat(sa))
    for epoch in range(batch_length):
        pred.append(self(torch.tensor(sa[epoch]).to(args.device)))
        #print(f"TN sa:{sa[epoch]}")
        #print(f"TN pred:{pred}")
        #print(f"TN action_value{action_value[epoch]}")
        #print(f"TN av tensor{torch.tensor((action_value[epoch],)).to(args.device)}")
    pred = torch.cat(pred)
    pred.to(args.device)
    #loss = criterion(pred, torch.tensor(action_value).to(args.device))  
    loss = criterion(pred, action_value.to(args.device))
    optimizer.zero_grad()
    loss.backward()
    torch.nn.utils.clip_grad_value_(self.parameters(), 100)
    optimizer.step()
    losses.append(loss.item())

    #pred=self(torch.cat(sa))
    """for epoch in range(batch_length):
        #pred.append(self(torch.tensor(sa[epoch]).to(args.device)))
        #print(f"TN sa:{sa[epoch]}")
        #print(f"TN pred:{pred}")
        #print(f"TN action_value{action_value[epoch]}")
        #print(f"TN av tensor{torch.tensor((action_value[epoch],)).to(args.device)}")
        #loss = criterion(pred, torch.tensor(action_value).to(args.device))  
        loss = criterion(self(torch.tensor(sa[epoch]).to(args.device)), action_value[epoch].to(args.device))  
        optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_value_(self.parameters(), 100)
        optimizer.step()
        losses.append(loss.item())"""
    return losses


def Target_Net_Processing(in_Pipe, export_net, queue_copy, stop_flag):
    time.sleep(1)
    #print(2)
    losses = []
    #raw_net=classes.QN(3,1)
    #net=classes.QN(3,1)
    net = classes.QN(2, 9)
    net.to(args.device)
    flag = 1
    """
    def train(self,mini_batch):
        criterion = torch.nn.MSELoss(reduction='mean')   
        optimizer = torch.optim.SGD(self.parameters(), lr=args.lr_rate)
        batch_length=len(mini_batch)
        sa, action_value= zip(*mini_batch)
        sa=list(sa)
        action_value=list(action_value)
        action_value=torch.cat(action_value)
        #next_states = np.array(next_states)
        pred=[]
        #pred=torch.zeros(batch_length,device=args.device)


        pred=self(torch.cat(sa))
        #for epoch in range(batch_length):
        #    pred.append(self(torch.tensor(sa[epoch],requires_grad=False).to(args.device)))
            #print(f"TN sa:{sa[epoch]}")
            #print(f"TN pred:{pred}")
            #print(f"TN action_value{action_value[epoch]}")
            #print(f"TN av tensor{torch.tensor((action_value[epoch],)).to(args.device)}")
        pred=torch.cat(pred)
        pred.to(args.device)
        #loss = criterion(pred, torch.tensor(action_value).to(args.device))  
        loss = criterion(pred, action_value.to(args.device))  
        optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_value_(self.parameters(), 100)
        optimizer.step()
        losses.append(float(f"{loss}"))
        return 1
    """
    while flag:
        time.sleep(0.02)
        while in_Pipe.empty():
            #print(f"stop_flag in TN{stop_flag.value}")
            if stop_flag.value:
                flag = 0
                break
            time.sleep(0.01)
        if not flag:
            plt.plot(losses)
            #plt.ylim(1)
            plt.savefig("loss plot.png")
            plt.close()
            break
        try:
            mini_batch = in_Pipe.get()  #mini_batch=[((row,column,action),(action_values))]
        except EOFError:
            return 0
        timer_start = time.time()
        #if not net.train(mini_batch):
        #    break
        losses = train(net, mini_batch, losses)
        period = time.time() - timer_start
        print(f"train time {period}")
        #print("training happended")
        #print(f"is copy queue full: {queue_copy.full()}")
        if queue_copy.full():
            queue_copy.get()
        queue_copy.put(copy.deepcopy(net).to("cpu"), block=0)
        #net.to(args.device)
    #export_net.put(copy.deepcopy(net))
    net.to("cpu")
    export_net.put(net)
    plt.ylim(0, 1)
    plt.plot(losses)
    plt.savefig("loss plot.png")
    plt.close()
    print("NT_Processing over")


def Active_Net_Processing(queue_sa, queue_out, queue_copy, stop_flag):
    #net=classes.QN(3,1)
    net = classes.QN(2, 9)
    net.to(args.device)
    #print(args.device)
    flag = True
    batch = []
    round_cnt = 0
    while flag:
        batch = []
        got = 0
        #print(4)
        while not got:
            try:
                sa_pair = queue_sa.get(0, 0.02)
                got = 1
            except queue.Empty:
                if stop_flag.value:
                    print("AN over")
                    return
                time.sleep(0.02)
                continue
        for sa in sa_pair:
            #print(f"sa: {sa}")
            #t1=time.time()
            out = net(sa).detach()
            #print("net cal take:",time.time()-t1)
            #out=net(torch.tensor(sa).to(device=args.device)).detach()
            #print(f"net outcome: {out}")
            batch.append(out)
        #print(f"sa: {sa}")
        """out=net(torch.tensor(sa_pair).to(device=args.device).detach()).detach()
        queue_out.put(out)
        #print(f"net outcome: {out}")
        #"""
        #print(queue_out.full())
        queue_out.put(batch)
        #print("PN Put")
        round_cnt += 1
        if round_cnt > args.update_round:
            round_cnt = 0
            try:
                net = queue_copy.get(0, 0.2)
                net.to(args.device)
                print("net upgraded")
            except EOFError:
                queue_out.close()
                break
            #queue_copy=multiprocessing.Queue()
            except queue.Empty:
                print("Empty raised")
                if stop_flag.value:
                    print("AN over")
                    return
                continue
    print("AN over")
    return


def Sample_Processing(space, max_row, max_column, queue_out, handle, queue_copy, stop_flag,
                      treading_num=args.sample_threading_num):
    classes.space = space
    args.max_column = max_column
    args.max_row = max_row
    state_num = len(space)
    global cache
    cache = []
    global cache_init
    cache_init = 1
    global upgraded
    upgraded = 0

    def sub_processing():
        global cache_init
        global cache
        global upgraded
        #print(3)
        #manager=multiprocessing.Manager()
        #Net_in_queue=manager.Queue(10)
        #Net_out_queue=manager.Queue(10)
        Net_in_queue = multiprocessing.Queue(2)
        Net_out_queue = multiprocessing.Queue(2)
        net_processing = multiprocessing.Process(target=Active_Net_Processing,
                                                 args=(Net_in_queue, Net_out_queue, queue_copy, stop_flag))
        net_processing.start()
        #print(5)
        epsilon_policy = classes.policy(args.epsilon_greedy(1))
        ##print(5)
        ##print(f"SP:{stop_flag.value}")
        ##print(f"AN:{stop_flag.value}")
        ##print(f"TN:{args.stop_flag_TN.value}")
        while not stop_flag.value:
            out = []
            #print(7)
            #print(f"cache length in thread {len(cache)}")
            init_state = random.randint(0, state_num - 1)
            #print(f"init_state {init_state}")
            #print(f"state_num {len(classes.space)}")
            action_values = []
            next_states = []
            eps = classes.episode(init_state, epsilon_policy, args.episode_length)
            for ind in range(len(eps.track) - 2, -1, -1):
                s, a = eps.track[ind]
                #x=s%args.max_column
                #y=s//args.max_column
                #next_s_n=eps.track[ind+1][0]
                #next_s=classes.space[eps.track[ind+1][0]]
                #next_x=next_s_n%args.max_column
                #next_y=next_s_n//args.max_column
                #sa_pairs=list(map(lambda a:(np.float32(next_x),np.float32(next_y),np.float32(a)),range(len(next_s.actions))))

                for a in range(len(classes.space[s].actions)):
                    next_s = next(classes.space[s].actions[a])
                    next_x = next_s % args.max_column
                    next_y = next_s // args.max_column
                    next_states.append((np.float32(next_x), np.float32(next_y)))
            while Net_in_queue.full():
                #print("full")
                if stop_flag.value:
                    print("sample sub processing over")
                    net_processing.join()
                    return
                time.sleep(0.1)
            Net_in_queue.put(torch.tensor(next_states).to(args.device))
            while Net_out_queue.empty():
                #print("empty")
                if stop_flag.value:
                    net_processing.join()
                    print("sample sub processing over")
                    return
                time.sleep(0.1)
            next_action_values = copy.deepcopy(list(Net_out_queue.get()))
            for ind in range(len(eps.track) - 1):
                action_values = []
                s = eps.track[ind][0]
                x = s % args.max_column
                y = s // args.max_column
                for a in range(len(classes.space[s].actions) - 1, -1, -1):
                    values = list(next_action_values.pop())
                    #print(values)
                    this_action_value = space[s].actions[a].action_reward + args.gamma * max(values).item()
                    #this_action_value=(this_action_value+space[s].actions[a].action_value)/2
                    this_action_value = space[s].actions[a].action_value
                    action_values = [this_action_value, ] + action_values
                #print(torch.tensor(action_values).detach())
                #out.append(((np.float32(x),np.float32(y)),torch.tensor(action_values).detach()))
                out.append(((np.float32(x), np.float32(y)), torch.tensor(
                    [np.float32(space[s].actions[a].action_value) for a in range(len(space[s].actions))]).detach()))
                #print(f"the reward of action{s}_{a}:{space[s].actions[a].action_reward}")
            print(next_action_values)
            if cache_init:
                print("init round")
                cache += random.sample(out, len(out))
                #print(cache)
                if len(cache) > args.AN_cache_min_length:
                    cache_init = 0
                continue

            #print(f"cache:{len(cache)}")
            #print(f"out:{len(out)}")

            position = random.sample(range(len(cache)), len(out))
            for i in range(len(out)):
                cache[position[i]] = out[i]
            upgraded = 1
            print("upgraded")
        net_processing.join()
        print("sample sub processing over")

    pool = ThreadPoolExecutor(treading_num)
    threadings = []
    event = threading.Event()
    for i in range(treading_num):
        #print("threading add")
        threadings.append(threading.Thread(target=sub_processing))
        threadings[i].start()
        time.sleep(5)
    #    threadings.append(pool.submit(sub_processing,i))
    while 1:
        try:
            if not handle.get() or stop_flag.value:
                handle.close()
                queue_out.close()
                break
        except EOFError:
            handle.close()
            queue_out.close()
            break
        while len(cache) < args.batch_length:
            #print(f"cache length{len(cache)}")
            event.wait(0.5)
        while not upgraded:
            if stop_flag:
                break
            event.wait(0.5)
        upgraded = 0
        #print("is queue out full:",queue_out.full())
        queue_out.put(random.sample(cache, args.batch_length))
        #print("data sent")
    for i in threadings:
        i.join()
    print("sample processing over")

```

## File: main.py
```
import Dqn
import classes
import args
import surroundings_function
import multiprocessing
import time
import numpy as np
import torch
import copy
import random

if __name__ == '__main__':
    classes.space = surroundings_function.import_maze(
        "maze.txt")
    print(len(classes.space))
    print(args.max_row, args.max_column)

    init_state = random.randint(0, len(classes.space) - 1)
    sign = 1
    # process_pool=multiprocessing.Pool()
    for cnt_round in range(1000):
        epsilon_policy = classes.policy(args.epsilon_greedy(1))
        print(cnt_round)
        # epsilon_policy=classes.policy(args.random_choice(5))

        init_state = random.randint(0, len(classes.space) - 1)
        eps = classes.episode(init_state, epsilon_policy, args.episode_length)
        # sample_return=classes.space[eps.track[-1][0]].actions[eps.track[-1][1]].action_value
        for ind in range(len(eps.track) - 2, 0, -1):
            s, a = eps.track[ind]

            # sample_return=classes.space[s].actions[a].action_reward+args.gamma*sample_return
            # classes.space[s].actions[a].update(sample_return,lambda x:0.001)
            classes.space[s].actions[a].update(classes.space[s].actions[a].action_reward + args.gamma * max(
                list(map(lambda x: x.action_value, classes.space[eps.track[ind + 1][0]].actions))))
    surroundings_function.plot_maze(classes.space, "target.png")

    # Queue_Sample_to_TN=multiprocessing.Queue(3)
    Pipe_Sample_to_TN_out, Pipe_Sample_to_TN_in = multiprocessing.Pipe(0)
    Queue_Sample_to_TN = multiprocessing.Queue(3)
    # handle_recv,handle_send=multiprocessing.Pipe(0)
    Queue_copy_net = multiprocessing.Queue(5)
    handle = multiprocessing.Queue(maxsize=1)
    net_export_out, net_export_in = multiprocessing.Pipe(0)
    net_export = multiprocessing.Queue(1)
    stop_flag = multiprocessing.Value("i", 0)

    Target_Net_Processing = multiprocessing.Process(target=Dqn.Target_Net_Processing,
                                                    args=(Queue_Sample_to_TN, net_export, Queue_copy_net, stop_flag))
    Sample_Processing = multiprocessing.Process(target=Dqn.Sample_Processing, args=(
    classes.space, args.max_row, args.max_column, Queue_Sample_to_TN, handle, Queue_copy_net, stop_flag))
    Sample_Processing.start()
    Target_Net_Processing.start()

    net_export_in.close()

    for i in range(args.run_round):
        handle.put(1)
        print(i, flush=1)
    stop_flag.value = 1
    handle.put(0)
    outcome_net = net_export.get()
    outcome_net.to(args.device)
    handle.close()
    # Target_Net_Processing.terminate()
    print("net got")
    state_num = len(classes.space)
    for s in range(state_num):

        x = np.float32(s % args.max_column)
        y = np.float32(s // args.max_column)
        action_values = list(outcome_net.forward(torch.tensor((x, y)).to(args.device)).to("cpu").detach())
        print(action_values)
        # classes.space[s].actions[a].action_value=np.float64(outcome_net.forward(torch.tensor((x,y,np.float32(a))).to(args.device)).detach().item())
        for a in range(len(classes.space[s].actions)):
            classes.space[s].actions[a].action_value = action_values[a]
            print(f"{s}_{a}/{state_num},action_value:{classes.space[s].actions[a].action_value}")

    surroundings_function.plot_maze(classes.space)
    print("maze printed")
    surroundings_function.out_figure()
    Target_Net_Processing.join()
    Sample_Processing.join()
    Target_Net_Processing.close()

```

## File: maze.py
```
import random
import sys


def two_dimensional_maze(width, height):
    maze = [[-1] * width for i in range(height)]
    directions = [(0, 1), (1, 0), (0, -1), (-1, 0)]

    def dfs(x, y):
        maze[y][x] = 0
        random.shuffle(directions)
        for dx, dy in directions:
            new_x, new_y = (x + 2 * dx + width) % width, (y + 2 * dy + height) % height
            if maze[new_y][new_x] == -1 and maze[(y + dy) % height][(x + dx) % width] != 1:
                maze[(y + dy) % height][(x + dx) % width] = 0
                dfs(new_x, new_y)

    target_x, target_y = random.randint(0, width - 1), random.randint(0, height - 1)
    maze[target_y][target_x] = 1
    random.shuffle(directions)
    dx_, dy_ = directions[0]
    new_x_, new_y_ = (target_x + dx_ + width) % width, (target_y + dy_ + height) % height
    dfs(new_x_, new_y_)
    return maze, (target_x, target_y)


if __name__ == '__main__':
    '''
    用于生成迷宫的代码
    输入迷宫的宽度和高度，生成的迷宫会保存在maze.txt文件中
    
    迷宫的格式为：
    
    第一行：迷宫的高度和宽度，用逗号分隔
    接下来的行：迷宫的每个格子，用空格分隔，0表示通路，-1表示墙，1表示目标点
    
    注意：迷宫的宽度和高度必须为奇数或者同时为偶数，否则会出bug
    '''

    # 注意：r和h必须同时为奇数或者同时为偶数，不然会出bug

    # r, h = int(input('width:')), int(input('height:'))
    r, h = 10, 10

    maze_, target = two_dimensional_maze(r, h)
    sys.stdout = open("maze.txt", "w")
    print(str(h) + "," + str(r))
    for row in maze_:
        print('\t'.join(map(str, row)))

    # print(target)

```

## File: maze.txt
```
10,10
-1	0	-1	-1	-1	-1	-1	0	-1	-1
-1	0	0	0	0	0	-1	0	0	0
-1	-1	-1	-1	-1	0	-1	-1	-1	0
0	0	0	0	-1	0	-1	0	-1	0
-1	-1	-1	0	-1	0	-1	0	-1	-1
0	0	-1	0	0	0	-1	0	0	0
-1	-1	-1	-1	-1	-1	-1	0	-1	0
-1	0	0	0	-1	0	0	0	-1	0
-1	0	-1	0	-1	0	-1	-1	-1	0
-1	0	-1	0	1	0	0	0	-1	0

```

## File: surroundings_function.py
```
import classes
import args
import numpy as np
import sys
import matplotlib.pyplot as plt
import math
import random


def import_maze(path: str) -> list:
    global max_column, max_row
    file = open(path, "r")
    n, m = tuple(map(lambda x: int(x), file.readline()[:-1].split(",")[:2]))
    max_row, max_column = n, m
    args.max_row, args.max_column = n, m
    blocks = [classes.block([]) for i in range(m) for j in range(n)]
    value = []

    def give_actions(pos: int) -> list:
        r, c = [pos // m, pos % m]
        a = [-1, 0, 1]
        # a=((0,0),(-1,0),(1,0),(0,-1),(0,1))
        ret = [classes.action(np.array([(r + i) % n * m + (c + j) % m, ]), np.array([1, ])) for i in a for j in a]
        # ret=[classes.action(np.array([(r+i)%n*m+(c+j)%m,]),np.array([1,]))for (i,j) in a]
        cnt = 0
        for i in a:
            for j in a:
                ret[cnt].action_reward = value[(r + i) % n * m + (c + j) % m]
                ret[cnt].action_value = value[(r + i) % n * m + (c + j) % m]
                ret[cnt].visit_cnt = 1
                ret[cnt].direction = (i, j)
                cnt += 1
        #for i,j in a:
        #    if value[(r+i)%n*m+(c+j)%m]<0:
        #        ret[cnt].next_state=[(r)%n*m+(c)%m,]
        #    ret[cnt].action_reward=value[(r+i)%n*m+(c+j)%m]
        #    ret[cnt].action_value=value[(r+i)%n*m+(c+j)%m]
        #    ret[cnt].visit_cnt=1
        #    cnt+=1
        return ret

    for i in range(n):
        lin = file.readline()[:-1].split("\t")
        value += tuple(map(lambda x: np.float64(x), lin))
    for i in range(n * m):
        blocks[i].actions = give_actions(i)
        blocks[i].state_reward = value[i]
    classes.space = classes.set(blocks, n, m)
    classes.space.maxlen = [n, m]
    return classes.space


def plot_maze(space, fig_name="actions taken.png"):
    # """
    # 绘制迷宫的可视化图形，根据要求展示不同元素及方向线段，并添加每个方向的action value显示
    # """
    # global max_column, max_row
    fig, ax = plt.subplots(figsize=(max_column, max_row))
    for i in range(len(space)):
        y = i // args.max_column
        x = i % args.max_column
        state = space[i]
        action_values = [action.action_value for action in state.actions]
        actions = [i for i in state.actions]
        # 找到最大动作值的索引
        max_value_index = np.argmax(action_values)
        a = [-1, 0, 1]
        directions = [(i, j) for i in a for j in
                      a]  # [(0, 1), (1, 0), (0, -1), (-1, 0), (-1, 1), (1, 1), (-1, -1), (1, -1)]
        for j in range(len(directions)):
            # dx, dy = directions[j]
            # k = next(actions[j])
            # _y = k // max_column
            # _x = k % max_column
            # _dx, _dy = _x - x, _y - y
            # if _dx == _dy == 0:
            #    continue
            dx, dy = actions[j].direction
            if dx == dy == 0:
                continue
            dy, dx = dx / math.sqrt(dx * dx + dy * dy), dy / math.sqrt(dx * dx + dy * dy)
            length = (1 + math.tanh(action_values[j])) * 0.1
            # 根据是否是最大动作值来确定颜色
            color = 'red' if j == max_value_index else 'green'
            ax.plot([x + 0.5, x + 0.5 + dx * length], [y + 0.5, y + 0.5 + dy * length], color=color)

            # 添加显示action value的文本，位置稍偏离线段起点一点，方便查看且不重叠
            # text_x = x + 0.5 + 0.05 * dx
            # text_y = y + 0.5 + 0.05 * dy
            # ax.text(text_x, text_y, f"{action_values[j]:.2f}", fontsize=8, color='black')

        if state.state_reward > 0:
            circle = plt.Circle((x + 0.5, y + 0.5), radius=0.3, color='blue')
            ax.add_patch(circle)
        elif state.state_reward < 0:
            rect = plt.Rectangle((x, y), width=1, height=1, alpha=0.8, color='yellow')
            ax.add_patch(rect)
        else:
            rect = plt.Rectangle((x, y), width=1, height=1, alpha=0.8, color='white')
            ax.add_patch(rect)

    ax.set_aspect('equal')
    ax.axis('off')
    plt.savefig(fig_name)
    plt.close()
    return


if __name__ == "__main__":
    max_column = 0
    max_row = 0
    sys.stdout = open("out.txt", "w")
    classes.space = import_maze("mazz.txt")
    for cnt_round in range(args.run_round):
        epsilon_policy = classes.policy(args.epsilon_greedy(1))

        # epsilon_policy=classes.policy(args.random_choice(5))

        init_state = random.randint(0, len(classes.space) - 1)
        eps = classes.episode(init_state, epsilon_policy, args.episode_lenth)
        # sample_return=classes.space[eps.track[-1][0]].actions[eps.track[-1][1]].action_value
        for ind in range(len(eps.track) - 2, 0, -1):
            s, a = eps.track[ind]

            # sample_return=classes.space[s].actions[a].action_reward+args.gamma*sample_return
            # classes.space[s].actions[a].update(sample_return,lambda x:0.001)
            classes.space[s].actions[a].update(classes.space[s].actions[a].action_reward + args.gamma * max(
                list(map(lambda x: x.action_value, classes.space[eps.track[ind + 1][0]].actions))))
    plot_maze()
    sys.stdout = sys.__stdout__


def out_figure():
    state_value = tuple(map(lambda x: max(tuple(map(lambda y: y.action_value, x.actions))), classes.space.blocks))
    fig1 = plt.figure(num=1, figsize=(max_column, max_row))
    axes1 = fig1.add_subplot(1, 1, 1)
    for i in range(len(classes.space)):
        y = i // max_column
        x = i % max_column
        #if not i%max_column:
        #    sys.stdout.write("\n")
        #sys.stdout.write(f"{state_value[i]:5.3f}\t")

        square = plt.Rectangle(xy=(1 / max_column * x, 1 / max_row * y), width=1 / max_column, height=1 / max_row,
                               alpha=0.8, angle=0.0, color=(
            max(0, -math.tanh(0.2 * (state_value[i] + classes.space[i].state_reward))), 0,
            max(0, math.tanh(0.2 * (state_value[i] + classes.space[i].state_reward))), 0))

        axes1.add_patch(square)

    plt.savefig("out.png")
    plt.close()

```
